{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking in Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/29 12:19:24 WARN Utils: Your hostname, codespaces-1f4fa0 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "23/05/29 12:19:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/05/29 12:19:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from splink.spark.jar_location import similarity_jar_location\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "\n",
    "conf = SparkConf()\n",
    "# This parallelism setting is only suitable for a small toy example\n",
    "conf.set(\"spark.driver.memory\", \"12g\")\n",
    "conf.set(\"spark.default.parallelism\", \"16\")\n",
    "\n",
    "\n",
    "# Add custom similarity functions, which are bundled with Splink\n",
    "# documented here: https://github.com/moj-analytical-services/splink_scalaudfs\n",
    "path = similarity_jar_location()\n",
    "conf.set(\"spark.jars\", path)\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "spark.sparkContext.setCheckpointDir(\"./tmp_checkpoints\")\n",
    "\n",
    "# Register the jaro winkler custom udf\n",
    "spark.udf.registerJavaFunction(\n",
    "    \"jaro_winkler\", \"uk.gov.moj.dash.linkage.JaroWinklerSimilarity\", types.DoubleType()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = spark.read.csv(\"./data/fake_1000.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splink.spark.spark_comparison_library as cl\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparisons\": [\n",
    "        cl.jaro_winkler_at_thresholds(\"first_name\", 0.8),\n",
    "        cl.jaro_winkler_at_thresholds(\"surname\", 0.8),\n",
    "        cl.levenshtein_at_thresholds(\"dob\"),\n",
    "        cl.exact_match(\"city\", term_frequency_adjustments=True),\n",
    "        cl.levenshtein_at_thresholds(\"email\"),\n",
    "    ],\n",
    "    \"blocking_rules_to_generate_predictions\": [\n",
    "        \"l.first_name = r.first_name\",\n",
    "        \"l.surname = r.surname\",\n",
    "    ],\n",
    "    \"retain_matching_columns\": True,\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "    \"em_convergence\": 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "Probability two random records match is estimated to be  0.00389.\n",
      "This means that amongst all possible pairwise record comparisons, one in 257.25 are expected to match.  With 499,500 total possible comparisons, we expect a total of around 1,941.67 matching pairs\n"
     ]
    }
   ],
   "source": [
    "from splink.spark.spark_linker import SparkLinker\n",
    "linker = SparkLinker(df, settings)\n",
    "deterministic_rules = [\n",
    "    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\",\n",
    "    \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\",\n",
    "    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\",\n",
    "    \"l.email = r.email\"\n",
    "]\n",
    "\n",
    "linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o115.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m linker\u001b[39m.\u001b[39;49mestimate_u_using_random_sampling(target_rows\u001b[39m=\u001b[39;49m\u001b[39m5e5\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/splink/linker.py:768\u001b[0m, in \u001b[0;36mLinker.estimate_u_using_random_sampling\u001b[0;34m(self, target_rows)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mestimate_u_using_random_sampling\u001b[39m(\u001b[39mself\u001b[39m, target_rows: \u001b[39mint\u001b[39m):\n\u001b[1;32m    742\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Estimate the u parameters of the linkage model using random sampling.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \n\u001b[1;32m    744\u001b[0m \u001b[39m    The u parameters represent the proportion of record comparisons that fall\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[39m        and returns nothing.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialise_df_concat_with_tf(materialise\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    769\u001b[0m     estimate_u_values(\u001b[39mself\u001b[39m, target_rows)\n\u001b[1;32m    770\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_populate_m_u_from_trained_values()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/splink/linker.py:292\u001b[0m, in \u001b[0;36mLinker._initialise_df_concat_with_tf\u001b[0;34m(self, materialise)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[39mif\u001b[39;00m materialise:\n\u001b[0;32m--> 292\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_pipeline(materialise_as_hash\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/splink/linker.py:346\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[0;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[1;32m    343\u001b[0m output_tablename_templated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mqueue[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput_table_name\n\u001b[1;32m    345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sql_to_splink_dataframe_checking_cache(\n\u001b[1;32m    347\u001b[0m         sql_gen,\n\u001b[1;32m    348\u001b[0m         output_tablename_templated,\n\u001b[1;32m    349\u001b[0m         materialise_as_hash,\n\u001b[1;32m    350\u001b[0m         use_cache,\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[39mexcept\u001b[39;00m Error \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    353\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/splink/linker.py:427\u001b[0m, in \u001b[0;36mLinker._sql_to_splink_dataframe_checking_cache\u001b[0;34m(self, sql, output_tablename_templated, materialise_as_hash, use_cache)\u001b[0m\n\u001b[1;32m    423\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_sql_against_backend(\n\u001b[1;32m    424\u001b[0m         sql, output_tablename_templated, table_name_hash\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_against_backend(\n\u001b[1;32m    428\u001b[0m         sql,\n\u001b[1;32m    429\u001b[0m         output_tablename_templated,\n\u001b[1;32m    430\u001b[0m         output_tablename_templated,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_names_of_tables_created_by_splink\u001b[39m.\u001b[39mappend(splink_dataframe\u001b[39m.\u001b[39mphysical_name)\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug_mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/splink/spark/spark_linker.py:264\u001b[0m, in \u001b[0;36mSparkLinker._execute_sql_against_backend\u001b[0;34m(self, sql, templated_name, physical_name)\u001b[0m\n\u001b[1;32m    261\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m5\u001b[39m, log_sql(sql))\n\u001b[1;32m    262\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39msql(sql)\n\u001b[0;32m--> 264\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_break_lineage_and_repartition(\n\u001b[1;32m    265\u001b[0m     spark_df, templated_name, physical_name\n\u001b[1;32m    266\u001b[0m )\n\u001b[1;32m    268\u001b[0m \u001b[39m# After blocking, want to repartition\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m# if templated\u001b[39;00m\n\u001b[1;32m    270\u001b[0m spark_df\u001b[39m.\u001b[39mcreateOrReplaceTempView(physical_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/splink/spark/spark_linker.py:247\u001b[0m, in \u001b[0;36mSparkLinker._break_lineage_and_repartition\u001b[0;34m(self, spark_df, templated_name, physical_name)\u001b[0m\n\u001b[1;32m    245\u001b[0m checkpoint_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_checkpoint_dir_path(spark_df)\n\u001b[1;32m    246\u001b[0m write_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_dir, physical_name)\n\u001b[0;32m--> 247\u001b[0m spark_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(write_path)\n\u001b[1;32m    248\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mparquet(write_path)\n\u001b[1;32m    249\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrote \u001b[39m\u001b[39m{\u001b[39;00mtemplated_name\u001b[39m}\u001b[39;00m\u001b[39m to parquet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o115.parquet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52594)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 274, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52610)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52626)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52642)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd4 in position 11: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52648)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8f in position 17: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52656)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xaf in position 4: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52670)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcb in position 11: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52682)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xaf in position 4: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52696)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf6 in position 11: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52698)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 14: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52712)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52716)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd9 in position 11: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 52726)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 279, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 268, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 4: invalid start byte\n",
      "----------------------------------------\n",
      "Bad pipe message: %s [b'\\x94@\\x00\\xe8\\xee\\x11\\xce\\xdem(\\xd0b\\xe5/@\\xc8z\\xc9 ,\\xda~g5']\n",
      "Bad pipe message: %s [b'`\\x8c\\r\\xd2g\\x07\\xf4\\xff@\\x1aO%\\xf1F[\\xafsU~\\xaf\\xff\\x08\\xd3\\xf8;\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06']\n",
      "Bad pipe message: %s [b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xb2\\xcb\\r\\xaa\\x02\\x05\\xfd&5\\x10\\xff\\xc4\\x8d\\xa9\\xcc\\x08\\x99w\\xd2\\xda\\xf5\\x8d']\n",
      "Bad pipe message: %s [b',k\\xc9Y\\xa8&\\x81\\xec2\\x89{(\\x80=\\xablZZ \\xef\\t%\\xe2\\xf2\\xe7\\xb3\\xa2\\xe1\\x08\\xab&i+\\x9b\\x97\\x13\\xea\\x82\\xa7w\\xf7\\x19Qk\\x97F,95\\x9a\\xc2\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00']\n",
      "Bad pipe message: %s [b'#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03']\n",
      "Bad pipe message: %s [b'\\x08\\x08\\x08\\t\\x08\\n\\x08', b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 ?\\x8c&t\\x84x\\x03#\\rJ\\x86tB\\x05y\\xcfg\\xe4TNV\\xea']\n",
      "Bad pipe message: %s [b'|\\x7f\\xe3\\x02\\x95\\x97\\x1c\\xd4\\x02>\\x1cm\\n\\x0bg\\x82H\\xf1\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0']\n",
      "Bad pipe message: %s [b'\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x00']\n",
      "Bad pipe message: %s [b'\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t12']\n",
      "Bad pipe message: %s [b'0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01']\n",
      "Bad pipe message: %s [b\"ST\\xabE+2\\x04\\x9c\\x13\\x0b\\x8d'\\xb1&\\x03\\x97\\x84E\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\", b':\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00']\n",
      "Bad pipe message: %s [b'\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00']\n",
      "Bad pipe message: %s [b'\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00']\n",
      "Bad pipe message: %s [b'?\\x90\\x1av@\\x93\\xd5\\xd8d\\x91m\\xb0\\xa7\\x91\\x98!\\xf3\\x08\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00']\n",
      "Bad pipe message: %s [b'\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00']\n",
      "Bad pipe message: %s [b'\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00']\n"
     ]
    }
   ],
   "source": [
    "linker.estimate_u_using_random_sampling(target_rows=5e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.first_name = r.first_name and l.surname = r.surname\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - dob\n",
      "    - city\n",
      "    - email\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - first_name\n",
      "    - surname\n",
      "                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 14:39:20 WARN DataSource: All paths were ignored:\n",
      "  file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/06909547-db7a-49ae-a2d5-e913b503cb7c/__splink__df_comparison_vectors_77ebe74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1: Largest change in params was -0.53 in the m_probability of dob, level `Exact match`\n",
      "Iteration 2: Largest change in params was 0.0335 in probability_two_random_records_match\n",
      "Iteration 3: Largest change in params was 0.0129 in probability_two_random_records_match\n",
      "Iteration 4: Largest change in params was 0.00639 in probability_two_random_records_match\n",
      "\n",
      "EM converged after 4 iterations\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - first_name (no m values are trained).\n",
      "    - surname (no m values are trained).\n",
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.dob = r.dob\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - first_name\n",
      "    - surname\n",
      "    - city\n",
      "    - email\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - dob\n",
      "                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 14:39:26 WARN DataSource: All paths were ignored:\n",
      "  file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/06909547-db7a-49ae-a2d5-e913b503cb7c/__splink__df_comparison_vectors_2bd95d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1: Largest change in params was -0.413 in the m_probability of surname, level `Exact match`\n",
      "Iteration 2: Largest change in params was 0.108 in probability_two_random_records_match\n",
      "Iteration 3: Largest change in params was 0.0348 in probability_two_random_records_match\n",
      "Iteration 4: Largest change in params was 0.0133 in probability_two_random_records_match\n",
      "Iteration 5: Largest change in params was 0.00593 in probability_two_random_records_match\n",
      "\n",
      "EM converged after 5 iterations\n",
      "\n",
      "Your model is fully trained. All comparisons have at least one estimate for their m and u values\n"
     ]
    }
   ],
   "source": [
    "training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\n",
    "training_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)\n",
    "\n",
    "training_blocking_rule = \"l.dob = r.dob\"\n",
    "training_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 14:39:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/09/19 14:39:32 WARN DataSource: All paths were ignored:\n",
      "  file:/Users/robinlinacre/Documents/data_linking/splink_demos/tmp_checkpoints/06909547-db7a-49ae-a2d5-e913b503cb7c/__splink__df_predict_99e9cb4\n"
     ]
    }
   ],
   "source": [
    "results = linker.predict(threshold_match_probability=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_weight</th>\n",
       "      <th>match_probability</th>\n",
       "      <th>unique_id_l</th>\n",
       "      <th>unique_id_r</th>\n",
       "      <th>first_name_l</th>\n",
       "      <th>first_name_r</th>\n",
       "      <th>gamma_first_name</th>\n",
       "      <th>bf_first_name</th>\n",
       "      <th>surname_l</th>\n",
       "      <th>surname_r</th>\n",
       "      <th>...</th>\n",
       "      <th>gamma_city</th>\n",
       "      <th>tf_city_l</th>\n",
       "      <th>tf_city_r</th>\n",
       "      <th>bf_city</th>\n",
       "      <th>bf_tf_adj_city</th>\n",
       "      <th>email_l</th>\n",
       "      <th>email_r</th>\n",
       "      <th>gamma_email</th>\n",
       "      <th>bf_email</th>\n",
       "      <th>match_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.371739</td>\n",
       "      <td>0.911904</td>\n",
       "      <td>220</td>\n",
       "      <td>223</td>\n",
       "      <td>Logan</td>\n",
       "      <td>Logan</td>\n",
       "      <td>2</td>\n",
       "      <td>86.748396</td>\n",
       "      <td>serguFon</td>\n",
       "      <td>Ferguson</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.212792</td>\n",
       "      <td>0.212792</td>\n",
       "      <td>10.316002</td>\n",
       "      <td>0.259162</td>\n",
       "      <td>l.feruson46@sahh.com</td>\n",
       "      <td>None</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.743406</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>879</td>\n",
       "      <td>880</td>\n",
       "      <td>Leo</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2</td>\n",
       "      <td>86.748396</td>\n",
       "      <td>Webster</td>\n",
       "      <td>Webster</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>0.008610</td>\n",
       "      <td>10.316002</td>\n",
       "      <td>6.404996</td>\n",
       "      <td>leo.webster54@moore.biez</td>\n",
       "      <td>None</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.140266</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>446</td>\n",
       "      <td>450</td>\n",
       "      <td>Aisha</td>\n",
       "      <td>Aisha</td>\n",
       "      <td>2</td>\n",
       "      <td>86.748396</td>\n",
       "      <td>Bryant</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011070</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.456259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aishab64@obrien-flores.com</td>\n",
       "      <td>aishab64@obrien-flores.com</td>\n",
       "      <td>3</td>\n",
       "      <td>257.458944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.829126</td>\n",
       "      <td>0.997806</td>\n",
       "      <td>446</td>\n",
       "      <td>448</td>\n",
       "      <td>Aisha</td>\n",
       "      <td>Aisha</td>\n",
       "      <td>2</td>\n",
       "      <td>86.748396</td>\n",
       "      <td>Bryant</td>\n",
       "      <td>BryBant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011070</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.456259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aishab64@obrien-flores.com</td>\n",
       "      <td>aishab64@obrien-flores.com</td>\n",
       "      <td>3</td>\n",
       "      <td>257.458944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.584844</td>\n",
       "      <td>0.989690</td>\n",
       "      <td>790</td>\n",
       "      <td>791</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>2</td>\n",
       "      <td>86.748396</td>\n",
       "      <td>Fisreh</td>\n",
       "      <td>Fishier</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.456259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>j.fisher4@sullivan.com</td>\n",
       "      <td>None</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_weight  match_probability unique_id_l unique_id_r first_name_l  \\\n",
       "0      3.371739           0.911904         220         223        Logan   \n",
       "1     14.743406           0.999964         879         880          Leo   \n",
       "2     13.140266           0.999889         446         450        Aisha   \n",
       "3      8.829126           0.997806         446         448        Aisha   \n",
       "4      6.584844           0.989690         790         791      Jackson   \n",
       "\n",
       "  first_name_r  gamma_first_name  bf_first_name surname_l surname_r  ...  \\\n",
       "0        Logan                 2      86.748396  serguFon  Ferguson  ...   \n",
       "1          Leo                 2      86.748396   Webster   Webster  ...   \n",
       "2        Aisha                 2      86.748396    Bryant      None  ...   \n",
       "3        Aisha                 2      86.748396    Bryant   BryBant  ...   \n",
       "4      Jackson                 2      86.748396    Fisreh   Fishier  ...   \n",
       "\n",
       "   gamma_city  tf_city_l tf_city_r    bf_city  bf_tf_adj_city  \\\n",
       "0           1   0.212792  0.212792  10.316002        0.259162   \n",
       "1           1   0.008610  0.008610  10.316002        6.404996   \n",
       "2           0   0.011070  0.001230   0.456259        1.000000   \n",
       "3           0   0.011070  0.001230   0.456259        1.000000   \n",
       "4           0   0.009840  0.001230   0.456259        1.000000   \n",
       "\n",
       "                      email_l                     email_r gamma_email  \\\n",
       "0        l.feruson46@sahh.com                        None          -1   \n",
       "1    leo.webster54@moore.biez                        None          -1   \n",
       "2  aishab64@obrien-flores.com  aishab64@obrien-flores.com           3   \n",
       "3  aishab64@obrien-flores.com  aishab64@obrien-flores.com           3   \n",
       "4      j.fisher4@sullivan.com                        None          -1   \n",
       "\n",
       "     bf_email  match_key  \n",
       "0    1.000000          0  \n",
       "1    1.000000          0  \n",
       "2  257.458944          0  \n",
       "3  257.458944          0  \n",
       "4    1.000000          0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.as_pandas_dataframe(limit=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
